# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtF2DILC9yckog8oohdMBcqLuaiqle5u
"""

import pandas as pd
import numpy as np

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# T√©l√©chargement des ressources NLTK n√©cessaires
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc
import time
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_excel('/content/data_tweets2.xlsx', sheet_name='Sheet 1 - sent_train')
df

# Fonction de nettoyage du texte
def clean_text(text):
    # Conversion en minuscules
    text = text.lower()

    # Suppression des URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Suppression des mentions Twitter et hashtags
    text = re.sub(r'@\w+|\$\w+|#\w+', '', text)

    # Suppression des caract√®res sp√©ciaux et chiffres
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    # Tokenization (d√©coupe le texte en une liste de mots)
    tokens = word_tokenize(text)

    # Suppression des stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization (R√©duit les mots √† leur forme de base pour √©viter les variations)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return ' '.join(tokens)

# Application du nettoyage aux tweets
df['cleaned_text'] = df['text'].apply(clean_text)
df

df = df.iloc[:, [1, 2]]  # Garde seulement la 2·µâ et 3·µâ colonne
df = df.iloc[:, [1, 0]]  # Inverse l'ordre des colonnes restante
df.head(20)

# Conversion en features avec TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(df['cleaned_text'])

# Pr√©paration des labels pour classification binaire (n√©gatif vs reste)
y_binary = (df['label'] == 0).astype(int)

"""Pourquoi faire √ßa ?
‚úÖ 1. Simplifier le probl√®me
Un mod√®le binaire est plus simple et souvent plus performant qu'un mod√®le multi-classes.
On peut utiliser des algorithmes standards comme :
R√©gression logistique binaire
SVM binaire
Random Forest binaire

‚úÖ 2. Cas d'usage courant : D√©tection des tweets n√©gatifs
Si on veut pr√©dire si un tweet est n√©gatif ou non, cette approche est parfaite.
Par exemple, une entreprise peut vouloir :
Identifier les clients m√©contents sur Twitter.
D√©tecter les commentaires n√©gatifs pour am√©liorer son service client.

‚úÖ 3. √âviter le d√©s√©quilibre des classes
Si la classe "neutre" ou "positive" est trop surdimensionn√©e, un mod√®le multi-class risque de mal apprendre.
En fusionnant neutre et positif en une seule classe (0), on √©quilibre mieux les donn√©es.
"""

# Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

print("Dimensions des donn√©es d'entra√Ænement:", X_train.shape)
print("Dimensions des donn√©es de test:", X_test.shape)
print(X_train[:5].toarray())  # Convertir la matrice en tableau pour affichage
print(np.unique(y_train, return_counts=True))  # Voir la r√©partition des classes

"""* X ‚Üí Les caract√©ristiques (features) de tes donn√©es (par exemple, du texte transform√© en vecteurs TF-IDF).
* y_binary ‚Üí Les √©tiquettes (labels) associ√©es (par exemple, 0 = n√©gatif, 1 = positif).
* X_train et X_test ‚Üí Ce sont des sous-ensembles de X apr√®s division.
* y_train et y_test ‚Üí Ce sont les √©tiquettes correspondantes √† X_train et X_test.

* 7634 documents pour l'entra√Ænement
* 1909 pour le test
* chaque document est repr√©sent√© par 10000 caract√©ristiques TF-IDF

---
Pourquoi faire un split ?

 √âviter le surapprentissage : Si on entra√Æne et teste sur les m√™mes donn√©es, le mod√®le pourrait juste "m√©moriser" les exemples.

  √âvaluer la performance : L'ensemble de test permet de mesurer si le mod√®le g√©n√©ralise bien sur de nouvelles donn√©es.

diviser les donn√©es en un ensemble d'entra√Ænement (80%) et un ensemble de test(20%) pour entra√Æner et √©valuer un mod√®le de machine learning

**random_state=42** assure que la division est r√©p√©table (pour obtenir les m√™mes r√©sultats √† chaque ex√©cution).

* **X** : Les features (ici, la matrice TF-IDF du texte).
* **y_binary**: Les √©tiquettes (ex. : 0 ou 1 si c'est un probl√®me de classification binaire).
* **X_train**: 80% des donn√©es pour entra√Æner le mod√®le.
* **X_test** : 20% des donn√©es pour tester le mod√®le.
* **y_train**: Les √©tiquettes associ√©es √† X_train.
* **y_test** : Les √©tiquettes associ√©es √† X_test.
"""

# Affichage des premiers exemples nettoy√©s
print("\nExemples de tweets nettoy√©s:")
print(df['cleaned_text'].head())

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    # Mesure du temps d'ex√©cution
    start_time = time.time()

    # Entra√Ænement du mod√®le
    model.fit(X_train, y_train)

    # Pr√©dictions
    y_pred = model.predict(X_test)

    # Temps d'ex√©cution
    execution_time = time.time() - start_time

    # M√©triques
    print(f"\nR√©sultats pour {model_name}")
    print("Temps d'ex√©cution: {:.2f} secondes".format(execution_time))
    print("\nRapport de classification:")
    print(classification_report(y_test, y_pred))

# D√©finition des mod√®les
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500),
    'SVM': SVC(),  # Mod√®le suppl√©mentaire 1
    'Naive Bayes': MultinomialNB()  # Mod√®le suppl√©mentaire 2
}

# √âvaluation de chaque mod√®le
for model_name, model in models.items():
    evaluate_model(model, X_train, X_test, y_train, y_test, model_name)

"""* Precision:	% de pr√©dictions correctes parmi celles qui ont √©t√© class√©es dans cette cat√©gorie.(Quand le mod√®le dit "0", il a raison 87% du temps)
* Recall:	% d'√©l√©ments d'une classe correctement identifi√©s.(0.20 ‚Üí Il n'a trouv√© que 20% des vrais "0")
* F1-score:	Moyenne harmonique entre pr√©cision et rappel.
* Support:	Nombre total d'exemples dans cette classe.
__________________
* Accuracy = 0.87 ‚Üí Le mod√®le classe correctement 87% des √©chantillons
* Macro avg (moyenne des classes) ‚Üí Score √©quilibr√© mais faible pour le rappel (0.60).
* Weighted avg (pond√©r√© par le support) ‚Üí Donne plus d'importance √† la classe majoritaire (0), d'o√π un bon f1-score global.

1Ô∏è‚É£ Logistic Regression

Temps d'ex√©cution: ‚è±Ô∏è 0.14 secondes R√©sultats:

Precision (classe 1): 92%
Recall (classe 1): 19% (tr√®s faible)
F1-score (classe 1): 32%
Accuracy globale: 87%

Interpr√©tation:

La r√©gression logistique classifie bien la classe majoritaire (0) avec un recall de 100%.
Par contre, elle ne d√©tecte presque pas la classe 1 (recall = 19%). Cela signifie que beaucoup d'exemples de classe 1 sont class√©s √† tort comme 0.
Ce mod√®le est biais√© en faveur de la classe majoritaire.

2Ô∏è‚É£ K-Nearest Neighbors (KNN)

Temps d'ex√©cution: ‚è±Ô∏è 0.30 secondes R√©sultats:

Precision (classe 1): 86%
Recall (classe 1): 6% (tr√®s faible)
F1-score (classe 1): 11%
Accuracy globale: 85%

Interpr√©tation:

KNN a du mal avec la classe minoritaire (1) : il ne d√©tecte presque aucun exemple de cette classe (recall de seulement 6%).
Cela signifie que presque tous les exemples de classe 1 sont mal class√©s comme 0.
KNN a tendance √† sur-apprendre les classes majoritaires, surtout si les donn√©es sont d√©s√©quilibr√©es.

3Ô∏è‚É£ Random Forest

Temps d'ex√©cution: ‚è±Ô∏è 11.80 secondes R√©sultats:

Precision (classe 1): 83%
Recall (classe 1): 32%
F1-score (classe 1): 46%
Accuracy globale: 88%

Interpr√©tation:

Random Forest fait mieux que KNN et Logistic Regression sur la classe 1 (recall de 32%, contre 6% pour KNN et 19% pour Logistic Regression).
Son temps d'ex√©cution est plus long car il entra√Æne 100 arbres de d√©cision.
Bilan : Un bon √©quilibre, mais il reste biais√© vers la classe majoritaire.

4Ô∏è‚É£ Neural Network (MLPClassifier)

Temps d'ex√©cution: ‚è±Ô∏è 53.99 secondes R√©sultats:

Precision (classe 1): 67%
Recall (classe 1): 48%
F1-score (classe 1): 56%
Accuracy globale: 88%

Interpr√©tation:

Ce mod√®le fait mieux que tous les autres sur la classe minoritaire (1) avec un recall de 48%.
Son temps d'ex√©cution est long car un r√©seau de neurones fait plusieurs passes sur les donn√©es (backpropagation).
Il a une meilleure capacit√© de g√©n√©ralisation mais peut n√©cessiter encore plus d'optimisation (nombre de couches, nombre d'it√©rations, r√©gularisation).

5Ô∏è‚É£ Support Vector Machine (SVM)

Temps d'ex√©cution: ‚è±Ô∏è 3.01 secondes R√©sultats:

Precision (classe 1): 95%
Recall (classe 1): 26%
F1-score (classe 1): 40%
Accuracy globale: 88%

Interpr√©tation:

SVM a une tr√®s bonne pr√©cision sur la classe minoritaire (95%), mais son recall est faible (26%).
Cela signifie qu'il classe bien les exemples de classe 1 quand il les d√©tecte, mais il en rate beaucoup.
Son temps d'ex√©cution est raisonnable par rapport √† Random Forest et Neural Network.

6Ô∏è‚É£ Naive Bayes

Temps d'ex√©cution: ‚è±Ô∏è 0.00 secondes (extr√™mement rapide) R√©sultats:

Precision (classe 1): 100%
Recall (classe 1): 2% (tr√®s mauvais)
F1-score (classe 1): 4%
Accuracy globale: 85%

Interpr√©tation:

Ce mod√®le pr√©dit la classe 0 presque tout le temps (recall de 2% pour la classe 1).
Il n'est pas adapt√© aux donn√©es d√©s√©quilibr√©es et suppose une ind√©pendance forte entre les mots (ce qui n'est pas r√©aliste pour du texte).
Son avantage est qu'il est tr√®s rapide (0.00 sec).

    Si tu veux de la rapidit√© : Logistic Regression ou Naive Bayes
    Si tu veux d√©tecter la classe 1 plus efficacement : Neural Network
    Si tu veux un bon compromis : Random Forest ou SVM

Le mod√®le Neural Network semble √™tre le meilleur pour √©quilibrer pr√©cision et recall, mais il est plus long √† entra√Æner.

#2 grading criterion 2: Model Optimization
##2.1 Selection of Explanatory Variables

#Neural Network model
"""

vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3) )  # features: caract√©ristiques #ngram: taille des groupes de mots (de 1 √† 3mots)
X = vectorizer.fit_transform(df['cleaned_text'])
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

"""üí° Probl√®me principal : d√©s√©quilibre de classes

La classe 1 est moins repr√©sent√©e (seulement 300 exemples contre 1609 pour la classe 0) ‚Üí le mod√®le penche fort vers la classe 0.
Nous allons donc r√©√©quilibrer:

##Sur-√©chantillonage  de la classe minoritaire
avec SMOTE
"""

!pip install -q imbalanced-learn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# SMOTE sur l'ensemble d'entra√Ænement
smote = SMOTE(random_state=42) #regarde les instances de la classe minoritaire dans y_train
#Il g√©n√®re artificiellement de nouveaux exemples synth√©tiques de cette classe en interpolant entre les exemples existants.
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
#les donn√©es sont r√©√©chantillonn√©es : la classe minoritaire est maintenant aussi repr√©sent√©e que la classe majoritaire.

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")

"""Il n'y a pas un grand chagement compar√© au mod√®le pr√©c√©dent, et il prend beaucoup plus de temps

#Sous-√©chantillonage de la classe majoritaire
avec la RandomUnderSampler
"""

from imblearn.under_sampling import RandomUnderSampler

#√©quilibre le jeu de donn√©es d√©s√©quilibr√© en r√©duisant la taille de la classe majoritaire (supprime de mani√®re al√©atoire des donn√©es de la classe majoritaire).
rus = RandomUnderSampler(random_state=42)
X_train_res, y_train_res = rus.fit_resample(X_train, y_train)

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (Undersampled)")

"""Le nouveau mod√®le est **meilleur si notre objectif est de ne pas rater les exemples de la classe 1 (fort recall).
Mais il est moins bon globalement et il pr√©dit trop souvent √† tort la classe 1 (faible pr√©cision).

‚úÖ Le recall de la classe 1 s‚Äôest nettement am√©lior√© : de 0.53 √† 0.74. Donc le mod√®le avec SMOTE d√©tecte beaucoup plus de vrais positifs pour la classe minoritaire.

‚ùå Mais la pr√©cision sur la classe 1 a chut√© : de 0.67 √† 0.39 ‚Üí beaucoup plus de faux positifs.

‚ùå Le f1-score de la classe 1 a baiss√© (de 0.59 √† 0.51) car la pr√©cision a trop chut√©.

‚ùå L‚Äôaccuracy globale a baiss√© (de 0.89 √† 0.77), car le mod√®le fait maintenant plus d‚Äôerreurs sur la classe majoritaire (0).

#Remplacer TfidfVectorizer par CountVectorizer
"""

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,3))
X = vectorizer.fit_transform(df['cleaned_text'])
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)
nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (Undersampled)")

"""##2.2 selection of hyperparameters"""

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=1000, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")

from sklearn.neural_network import MLPClassifier

nn_model = MLPClassifier(
    hidden_layer_sizes=(200, 100),
    max_iter=1000,
    random_state=42,
    alpha=0.0001,               # r√©gularisation L2 #plus alpha est grand plus le mod√®le sera "puni" pour les poids trop √©lev√©s ‚Üí moins de surapprentissage, mais possible sous-apprentissage si trop fort.
    activation='relu',          # ou 'tanh'(souvent pour du texte ), 'logistic', 'identity'
    learning_rate_init=0.001    # taux d‚Äôapprentissage initial
)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")