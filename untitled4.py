# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtF2DILC9yckog8oohdMBcqLuaiqle5u
"""

import pandas as pd
import numpy as np

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Téléchargement des ressources NLTK nécessaires
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc
import time
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_excel('/content/data_tweets2.xlsx', sheet_name='Sheet 1 - sent_train')
df

# Fonction de nettoyage du texte
def clean_text(text):
    # Conversion en minuscules
    text = text.lower()

    # Suppression des URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Suppression des mentions Twitter et hashtags
    text = re.sub(r'@\w+|\$\w+|#\w+', '', text)

    # Suppression des caractères spéciaux et chiffres
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    # Tokenization (découpe le texte en une liste de mots)
    tokens = word_tokenize(text)

    # Suppression des stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization (Réduit les mots à leur forme de base pour éviter les variations)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return ' '.join(tokens)

# Application du nettoyage aux tweets
df['cleaned_text'] = df['text'].apply(clean_text)
df

df = df.iloc[:, [1, 2]]  # Garde seulement la 2ᵉ et 3ᵉ colonne
df = df.iloc[:, [1, 0]]  # Inverse l'ordre des colonnes restante
df.head(20)

# Conversion en features avec TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(df['cleaned_text'])

# Préparation des labels pour classification binaire (négatif vs reste)
y_binary = (df['label'] == 0).astype(int)

"""Pourquoi faire ça ?
✅ 1. Simplifier le problème
Un modèle binaire est plus simple et souvent plus performant qu'un modèle multi-classes.
On peut utiliser des algorithmes standards comme :
Régression logistique binaire
SVM binaire
Random Forest binaire

✅ 2. Cas d'usage courant : Détection des tweets négatifs
Si on veut prédire si un tweet est négatif ou non, cette approche est parfaite.
Par exemple, une entreprise peut vouloir :
Identifier les clients mécontents sur Twitter.
Détecter les commentaires négatifs pour améliorer son service client.

✅ 3. Éviter le déséquilibre des classes
Si la classe "neutre" ou "positive" est trop surdimensionnée, un modèle multi-class risque de mal apprendre.
En fusionnant neutre et positif en une seule classe (0), on équilibre mieux les données.
"""

# Split des données
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

print("Dimensions des données d'entraînement:", X_train.shape)
print("Dimensions des données de test:", X_test.shape)
print(X_train[:5].toarray())  # Convertir la matrice en tableau pour affichage
print(np.unique(y_train, return_counts=True))  # Voir la répartition des classes

"""* X → Les caractéristiques (features) de tes données (par exemple, du texte transformé en vecteurs TF-IDF).
* y_binary → Les étiquettes (labels) associées (par exemple, 0 = négatif, 1 = positif).
* X_train et X_test → Ce sont des sous-ensembles de X après division.
* y_train et y_test → Ce sont les étiquettes correspondantes à X_train et X_test.

* 7634 documents pour l'entraînement
* 1909 pour le test
* chaque document est représenté par 10000 caractéristiques TF-IDF

---
Pourquoi faire un split ?

 Éviter le surapprentissage : Si on entraîne et teste sur les mêmes données, le modèle pourrait juste "mémoriser" les exemples.

  Évaluer la performance : L'ensemble de test permet de mesurer si le modèle généralise bien sur de nouvelles données.

diviser les données en un ensemble d'entraînement (80%) et un ensemble de test(20%) pour entraîner et évaluer un modèle de machine learning

**random_state=42** assure que la division est répétable (pour obtenir les mêmes résultats à chaque exécution).

* **X** : Les features (ici, la matrice TF-IDF du texte).
* **y_binary**: Les étiquettes (ex. : 0 ou 1 si c'est un problème de classification binaire).
* **X_train**: 80% des données pour entraîner le modèle.
* **X_test** : 20% des données pour tester le modèle.
* **y_train**: Les étiquettes associées à X_train.
* **y_test** : Les étiquettes associées à X_test.
"""

# Affichage des premiers exemples nettoyés
print("\nExemples de tweets nettoyés:")
print(df['cleaned_text'].head())

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    # Mesure du temps d'exécution
    start_time = time.time()

    # Entraînement du modèle
    model.fit(X_train, y_train)

    # Prédictions
    y_pred = model.predict(X_test)

    # Temps d'exécution
    execution_time = time.time() - start_time

    # Métriques
    print(f"\nRésultats pour {model_name}")
    print("Temps d'exécution: {:.2f} secondes".format(execution_time))
    print("\nRapport de classification:")
    print(classification_report(y_test, y_pred))

# Définition des modèles
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500),
    'SVM': SVC(),  # Modèle supplémentaire 1
    'Naive Bayes': MultinomialNB()  # Modèle supplémentaire 2
}

# Évaluation de chaque modèle
for model_name, model in models.items():
    evaluate_model(model, X_train, X_test, y_train, y_test, model_name)

"""* Precision:	% de prédictions correctes parmi celles qui ont été classées dans cette catégorie.(Quand le modèle dit "0", il a raison 87% du temps)
* Recall:	% d'éléments d'une classe correctement identifiés.(0.20 → Il n'a trouvé que 20% des vrais "0")
* F1-score:	Moyenne harmonique entre précision et rappel.
* Support:	Nombre total d'exemples dans cette classe.
__________________
* Accuracy = 0.87 → Le modèle classe correctement 87% des échantillons
* Macro avg (moyenne des classes) → Score équilibré mais faible pour le rappel (0.60).
* Weighted avg (pondéré par le support) → Donne plus d'importance à la classe majoritaire (0), d'où un bon f1-score global.

1️⃣ Logistic Regression

Temps d'exécution: ⏱️ 0.14 secondes Résultats:

Precision (classe 1): 92%
Recall (classe 1): 19% (très faible)
F1-score (classe 1): 32%
Accuracy globale: 87%

Interprétation:

La régression logistique classifie bien la classe majoritaire (0) avec un recall de 100%.
Par contre, elle ne détecte presque pas la classe 1 (recall = 19%). Cela signifie que beaucoup d'exemples de classe 1 sont classés à tort comme 0.
Ce modèle est biaisé en faveur de la classe majoritaire.

2️⃣ K-Nearest Neighbors (KNN)

Temps d'exécution: ⏱️ 0.30 secondes Résultats:

Precision (classe 1): 86%
Recall (classe 1): 6% (très faible)
F1-score (classe 1): 11%
Accuracy globale: 85%

Interprétation:

KNN a du mal avec la classe minoritaire (1) : il ne détecte presque aucun exemple de cette classe (recall de seulement 6%).
Cela signifie que presque tous les exemples de classe 1 sont mal classés comme 0.
KNN a tendance à sur-apprendre les classes majoritaires, surtout si les données sont déséquilibrées.

3️⃣ Random Forest

Temps d'exécution: ⏱️ 11.80 secondes Résultats:

Precision (classe 1): 83%
Recall (classe 1): 32%
F1-score (classe 1): 46%
Accuracy globale: 88%

Interprétation:

Random Forest fait mieux que KNN et Logistic Regression sur la classe 1 (recall de 32%, contre 6% pour KNN et 19% pour Logistic Regression).
Son temps d'exécution est plus long car il entraîne 100 arbres de décision.
Bilan : Un bon équilibre, mais il reste biaisé vers la classe majoritaire.

4️⃣ Neural Network (MLPClassifier)

Temps d'exécution: ⏱️ 53.99 secondes Résultats:

Precision (classe 1): 67%
Recall (classe 1): 48%
F1-score (classe 1): 56%
Accuracy globale: 88%

Interprétation:

Ce modèle fait mieux que tous les autres sur la classe minoritaire (1) avec un recall de 48%.
Son temps d'exécution est long car un réseau de neurones fait plusieurs passes sur les données (backpropagation).
Il a une meilleure capacité de généralisation mais peut nécessiter encore plus d'optimisation (nombre de couches, nombre d'itérations, régularisation).

5️⃣ Support Vector Machine (SVM)

Temps d'exécution: ⏱️ 3.01 secondes Résultats:

Precision (classe 1): 95%
Recall (classe 1): 26%
F1-score (classe 1): 40%
Accuracy globale: 88%

Interprétation:

SVM a une très bonne précision sur la classe minoritaire (95%), mais son recall est faible (26%).
Cela signifie qu'il classe bien les exemples de classe 1 quand il les détecte, mais il en rate beaucoup.
Son temps d'exécution est raisonnable par rapport à Random Forest et Neural Network.

6️⃣ Naive Bayes

Temps d'exécution: ⏱️ 0.00 secondes (extrêmement rapide) Résultats:

Precision (classe 1): 100%
Recall (classe 1): 2% (très mauvais)
F1-score (classe 1): 4%
Accuracy globale: 85%

Interprétation:

Ce modèle prédit la classe 0 presque tout le temps (recall de 2% pour la classe 1).
Il n'est pas adapté aux données déséquilibrées et suppose une indépendance forte entre les mots (ce qui n'est pas réaliste pour du texte).
Son avantage est qu'il est très rapide (0.00 sec).

    Si tu veux de la rapidité : Logistic Regression ou Naive Bayes
    Si tu veux détecter la classe 1 plus efficacement : Neural Network
    Si tu veux un bon compromis : Random Forest ou SVM

Le modèle Neural Network semble être le meilleur pour équilibrer précision et recall, mais il est plus long à entraîner.

#2 grading criterion 2: Model Optimization
##2.1 Selection of Explanatory Variables

#Neural Network model
"""

vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3) )  # features: caractéristiques #ngram: taille des groupes de mots (de 1 à 3mots)
X = vectorizer.fit_transform(df['cleaned_text'])
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

"""💡 Problème principal : déséquilibre de classes

La classe 1 est moins représentée (seulement 300 exemples contre 1609 pour la classe 0) → le modèle penche fort vers la classe 0.
Nous allons donc rééquilibrer:

##Sur-échantillonage  de la classe minoritaire
avec SMOTE
"""

!pip install -q imbalanced-learn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# SMOTE sur l'ensemble d'entraînement
smote = SMOTE(random_state=42) #regarde les instances de la classe minoritaire dans y_train
#Il génère artificiellement de nouveaux exemples synthétiques de cette classe en interpolant entre les exemples existants.
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
#les données sont rééchantillonnées : la classe minoritaire est maintenant aussi représentée que la classe majoritaire.

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")

"""Il n'y a pas un grand chagement comparé au modèle précédent, et il prend beaucoup plus de temps

#Sous-échantillonage de la classe majoritaire
avec la RandomUnderSampler
"""

from imblearn.under_sampling import RandomUnderSampler

#équilibre le jeu de données déséquilibré en réduisant la taille de la classe majoritaire (supprime de manière aléatoire des données de la classe majoritaire).
rus = RandomUnderSampler(random_state=42)
X_train_res, y_train_res = rus.fit_resample(X_train, y_train)

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (Undersampled)")

"""Le nouveau modèle est **meilleur si notre objectif est de ne pas rater les exemples de la classe 1 (fort recall).
Mais il est moins bon globalement et il prédit trop souvent à tort la classe 1 (faible précision).

✅ Le recall de la classe 1 s’est nettement amélioré : de 0.53 à 0.74. Donc le modèle avec SMOTE détecte beaucoup plus de vrais positifs pour la classe minoritaire.

❌ Mais la précision sur la classe 1 a chuté : de 0.67 à 0.39 → beaucoup plus de faux positifs.

❌ Le f1-score de la classe 1 a baissé (de 0.59 à 0.51) car la précision a trop chuté.

❌ L’accuracy globale a baissé (de 0.89 à 0.77), car le modèle fait maintenant plus d’erreurs sur la classe majoritaire (0).

#Remplacer TfidfVectorizer par CountVectorizer
"""

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,3))
X = vectorizer.fit_transform(df['cleaned_text'])
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)
nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (Undersampled)")

"""##2.2 selection of hyperparameters"""

nn_model = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=1000, random_state=42)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")

from sklearn.neural_network import MLPClassifier

nn_model = MLPClassifier(
    hidden_layer_sizes=(200, 100),
    max_iter=1000,
    random_state=42,
    alpha=0.0001,               # régularisation L2 #plus alpha est grand plus le modèle sera "puni" pour les poids trop élevés → moins de surapprentissage, mais possible sous-apprentissage si trop fort.
    activation='relu',          # ou 'tanh'(souvent pour du texte ), 'logistic', 'identity'
    learning_rate_init=0.001    # taux d’apprentissage initial
)
evaluate_model(nn_model, X_train_res, X_test, y_train_res, y_test, "Neural Network (SMOTE)")